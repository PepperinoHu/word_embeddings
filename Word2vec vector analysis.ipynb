{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI630 Homework 2: Word2vec Vector Analysis\n",
    "\n",
    "*Important Note:* Start this notebook only after you've gotten your word2vec model up and running!\n",
    "\n",
    "Many NLP packages support working with word embeddings. In this notebook you can work through the various problems assigned in Task 3. We've provided the basic functionality for loading word vectors using [Gensim](https://radimrehurek.com/gensim/models/keyedvectors.html), a good library for learning and using word vectors, and for working with the vectors. \n",
    "\n",
    "One of the fun parts of word vectors is getting a sense of what they learned. Feel free to explore the vectors here! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('your-saved-file-here.wv', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.60193205, -0.49068227,  0.07305328, -0.29008874, -0.17612962,\n",
       "       -0.6153505 ,  0.17692143,  0.4994871 ,  0.37855098, -0.31186864,\n",
       "        0.77269626,  0.5604661 , -0.00658537,  0.55655473, -0.6298628 ,\n",
       "        0.21582761,  0.2735747 , -0.03791024,  0.1148749 ,  0.27869835,\n",
       "        0.38620535, -0.34555018, -0.16305329, -0.27708077, -0.13301618,\n",
       "        0.08763094,  0.54521936,  0.10553864, -0.20269583, -0.00581928,\n",
       "       -0.37908912, -0.11814661, -0.16742116,  0.02892506, -0.58867705,\n",
       "       -0.4049593 ,  0.399659  , -0.2393625 , -0.4442917 ,  0.44781163,\n",
       "        0.5550385 ,  0.23669347, -0.32640332,  0.05137821,  0.5063348 ,\n",
       "       -0.18657202, -0.45645714,  0.06247716,  0.27713165,  0.32943398],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('novels', 0.8402631878852844),\n",
       " ('volumes', 0.8343730568885803),\n",
       " ('articles', 0.8070845603942871),\n",
       " ('stories', 0.800335168838501),\n",
       " ('novellas', 0.7956311106681824),\n",
       " ('travelogues', 0.7760347127914429),\n",
       " ('columns', 0.7728822231292725),\n",
       " ('anthologies', 0.7602205276489258),\n",
       " ('poems', 0.7594453692436218),\n",
       " ('monographs', 0.7567578554153442)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(a, b, c):\n",
    "    return word_vectors.most_similar(positive=[b, c], negative=[a])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tsar'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('man', 'woman', 'king')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
